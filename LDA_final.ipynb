{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DylNvDHO_ySu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "DylNvDHO_ySu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import random\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "Vl7Iz6bb_dxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "F1Ry4Qs1_3JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('sheet.xlsx')\n",
        "df1 = pd.read_excel('diag.xlsx')\n",
        "df1.rename(columns={'Diagnostic Procedure(s) Ordered':'procedure ordered'}, inplace=True)\n",
        "df.rename(columns={'Post_Op_Diagnosis | DESCRIPTION_OF_Procedure | Post_Op_Impression':'description'}, inplace=True)"
      ],
      "metadata": {
        "id": "g6tMtVtE_gRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "mkERlkHs_7rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens if token.isalpha()]\n",
        "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "df['description_clean'] = df['description'].apply(preprocess)\n",
        "df1['procedure_clean'] = df1['procedure ordered'].apply(preprocess)"
      ],
      "metadata": {
        "id": "JFxEs2gG_jjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "ONwU1kak__tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LDA model\n",
        "vectorizer = CountVectorizer(max_df=0.8, min_df=5)\n",
        "X = vectorizer.fit_transform(df1['procedure_clean'])\n",
        "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda_model.fit(X)\n",
        "\n",
        "def get_lda_similarity(text1, text2):\n",
        "    text1_vec = vectorizer.transform([preprocess(text1)])\n",
        "    text2_vec = vectorizer.transform([preprocess(text2)])\n",
        "    text1_topic_dist = lda_model.transform(text1_vec)[0]\n",
        "    text2_topic_dist = lda_model.transform(text2_vec)[0]\n",
        "    return 1 - nltk.cluster.util.cosine_distance(text1_topic_dist, text2_topic_dist)\n",
        "\n",
        "# Calculate LDA similarity for each combination of test data and comparison data\n",
        "df['expected_risk level'] = df['Gina selection']\n",
        "\n",
        "test_data = df['description_clean']\n",
        "comp_data = df1['procedure_clean']\n",
        "df['lda_score'] = pd.Series([get_lda_similarity(td, cd) for td, cd in zip(test_data, comp_data)])"
      ],
      "metadata": {
        "id": "YT-7aYXs_pIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Results"
      ],
      "metadata": {
        "id": "mh-E6fYaAC0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWh9O0Ga8m64",
        "outputId": "b7e4ae82-3cd1-44cb-f080-2ba5edc69274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "<ipython-input-2-f05506bc78ae>:91: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df_top10['GINA Selection'] = final_df_top10.groupby('description')['Gina selection'].transform(lambda x: ', '.join(set(x)))\n"
          ]
        }
      ],
      "source": [
        "# Rank by LDA score within each test data\n",
        "df['rank'] = df.groupby('description')['lda_score'].rank(ascending=False, method='dense')\n",
        "\n",
        "# Map expected risk level and GINA selection to numeric values\n",
        "def map_to_numeric(category):\n",
        "    if 'Minimal' in category or 'Straightforward' in category:\n",
        "        return 4\n",
        "    elif 'Low' in category:\n",
        "        return 3\n",
        "    elif 'Moderate' in category:\n",
        "        return 2\n",
        "    elif 'High' in category:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df['expected_risk level val'] = df['expected_risk level'].apply(map_to_numeric)\n",
        "df['gina selection val'] = df['Gina selection'].apply(map_to_numeric)\n",
        "\n",
        "df['ai selection'] = df.apply(lambda row: random.choice(row['Gina selection'].split(',')), axis=1)\n",
        "\n",
        "\n",
        "# Rank ai selection relative to Gina selection\n",
        "df['ai selection rank'] = df.groupby('Gina selection')['ai selection'].rank(ascending=False, method='dense')\n",
        "\n",
        "# Filter for top 10 matches for each test data\n",
        "final_df_top10 = df[df['rank'] <= 10]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle combined GINA selection values\n",
        "final_df_top10['GINA Selection'] = final_df_top10.groupby('description')['Gina selection'].transform(lambda x: ', '.join(set(x)))\n",
        "\n",
        "# Export final dataframe\n",
        "final_df_top10.to_excel('final_output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "B6GBb4-y_v2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}